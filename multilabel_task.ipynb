{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "##Imports\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import openpyxl\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Does augmenting classifiers with structured data help?\n",
    "# Do it for both the baseline classical ML and transformers then compare\n",
    "# Try it with just structured data [0s and 1s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Ti'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet Name: Sample\n",
      "Sheet Name: Sentence_Labeling\n",
      "Sheet Name: ADR_Identified\n",
      "Sheet Name: ADR_Mapped\n",
      "Sheet Name: WD_Identified\n",
      "Sheet Name: WD-Mapped \n",
      "Sheet Name: SSI_Identified\n",
      "Sheet Name: SSI_Mapped\n",
      "Sheet Name: DI_Identified\n",
      "Sheet Name: DI_Mapped\n"
     ]
    }
   ],
   "source": [
    "## Reading in the PSYTar data set and parsing it\n",
    "\n",
    "## Use sentence_labelling sheet\n",
    "\n",
    "fileName=\".\\ONLINE_FORA\\PsyTAR_dataset.xlsx\"\n",
    "data=pd.ExcelFile(fileName)\n",
    "sheets={}\n",
    "for sheet in data.sheet_names:\n",
    "    sheets[sheet]=data.parse(sheet)\n",
    "\n",
    "## Remove the first two sheets (License and read_me)\n",
    "sheets.pop('License',None)\n",
    "sheets.pop('read_me',None)\n",
    "\n",
    "## This will print out the sheet names for the whole excel\n",
    "for sheet in sheets.keys():\n",
    "    print(f\"Sheet Name: {sheet}\")\n",
    "\n",
    "#To access a sheet, perform sheet['Sheet_Name']; e.g., sheets['Sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2183)\t0.3560733570424169\n",
      "  (0, 2858)\t0.5335210557775061\n",
      "  (0, 2980)\t0.33695305203929327\n",
      "  (0, 4715)\t0.3577130194209486\n",
      "  (0, 4193)\t0.36113778160835414\n",
      "  (0, 2071)\t0.2627216248356759\n",
      "  (0, 5156)\t0.2390135798252401\n",
      "  (0, 1829)\t0.30083291559032393\n",
      "  (1, 2765)\t0.4585168729312828\n",
      "  (1, 1351)\t0.8886857021677111\n",
      "  (2, 2263)\t0.2708094866549267\n",
      "  (2, 4586)\t0.4316846093641784\n",
      "  (2, 5071)\t0.44358904667953863\n",
      "  (2, 4827)\t0.2570546169681927\n",
      "  (2, 3078)\t0.23875893121623276\n",
      "  (2, 4151)\t0.3207489298964512\n",
      "  (2, 1492)\t0.29911785906957045\n",
      "  (2, 1201)\t0.35264280744022497\n",
      "  (2, 4287)\t0.3220996041881179\n",
      "  (3, 4012)\t0.534375375933816\n",
      "  (3, 3326)\t0.5729355939201095\n",
      "  (3, 1018)\t0.39488574070665183\n",
      "  (3, 1237)\t0.3054771887014283\n",
      "  (3, 5)\t0.3700495936825698\n",
      "  (4, 4801)\t0.3601630582861626\n",
      "  :\t:\n",
      "  (6006, 3810)\t0.266723700884449\n",
      "  (6006, 278)\t0.30278988091992165\n",
      "  (6006, 4033)\t0.24160419308016043\n",
      "  (6006, 1558)\t0.24160419308016043\n",
      "  (6006, 2821)\t0.21617383387506503\n",
      "  (6006, 3470)\t0.20987895271102364\n",
      "  (6006, 1390)\t0.23609327417139986\n",
      "  (6006, 2098)\t0.1721949331238259\n",
      "  (6006, 2792)\t0.15461917912599263\n",
      "  (6007, 5221)\t0.40690224630890753\n",
      "  (6007, 1388)\t0.36244934695018927\n",
      "  (6007, 2358)\t0.40690224630890753\n",
      "  (6007, 987)\t0.36244934695018927\n",
      "  (6007, 5165)\t0.28822766104713454\n",
      "  (6007, 2896)\t0.29736053264983103\n",
      "  (6007, 1619)\t0.26742602182403663\n",
      "  (6007, 1809)\t0.3151406469723878\n",
      "  (6007, 2777)\t0.25257333627086354\n",
      "  (6008, 3452)\t0.462967378880836\n",
      "  (6008, 4290)\t0.4133719291682648\n",
      "  (6008, 1695)\t0.41770974043639536\n",
      "  (6008, 3642)\t0.462967378880836\n",
      "  (6008, 186)\t0.3151272227530411\n",
      "  (6008, 2777)\t0.24953491233016006\n",
      "  (6008, 5269)\t0.25375538243161966\n",
      "Number of Drug Types: 4\n",
      "          id  comment_id        drug_id  sentence_index  \\\n",
      "0        1.0         1.0      lexapro.1             1.0   \n",
      "1        2.0         1.0      lexapro.1             2.0   \n",
      "2        3.0         1.0      lexapro.1             3.0   \n",
      "3        4.0         1.0      lexapro.1             4.0   \n",
      "4        5.0         1.0      lexapro.1             5.0   \n",
      "...      ...         ...            ...             ...   \n",
      "6004  1545.0       228.0  effexorxr.228            14.0   \n",
      "6005  1546.0       228.0  effexorxr.228            15.0   \n",
      "6006  1547.0       228.0  effexorxr.228            16.0   \n",
      "6007  1548.0       228.0  effexorxr.228            17.0   \n",
      "6008  1549.0       228.0  effexorxr.228            18.0   \n",
      "\n",
      "                                              sentences  ADR   WD   EF  INF  \\\n",
      "0     extreme weight gain short-term memory loss hai...  1.0  0.0  0.0  0.0   \n",
      "1                                      detoxing lexapro  0.0  0.0  0.0  0.0   \n",
      "2     slowly cut dosage several months took vitamin ...  0.0  0.0  0.0  0.0   \n",
      "3                          10 days completely omg rough  0.0  0.0  0.0  0.0   \n",
      "4     flu-like symptoms dizziness major mood swings ...  0.0  1.0  0.0  0.0   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "6004                                increase dosage yet  0.0  0.0  0.0  0.0   \n",
      "6005           'm hoping able stay 75 mgs long possible  0.0  0.0  0.0  0.0   \n",
      "6006  reading withdrawals little scarey like said pe...  0.0  0.0  0.0  0.0   \n",
      "6007  effexor made huge difference life come experie...  0.0  0.0  1.0  0.0   \n",
      "6008              would small price pay able enjoy life  0.0  0.0  1.0  0.0   \n",
      "\n",
      "      SSI DI  Findings  others  rating category  drug_name  \n",
      "0     0.0  0       0.0       0     1.0     ssri    lexapro  \n",
      "1     0.0  0       0.0       0     1.0     ssri    lexapro  \n",
      "2     0.0  0       0.0       1     1.0     ssri    lexapro  \n",
      "3     0.0  0       0.0       1     1.0     ssri    lexapro  \n",
      "4     0.0  0       0.0       0     1.0     ssri    lexapro  \n",
      "...   ... ..       ...     ...     ...      ...        ...  \n",
      "6004  0.0  0       0.0       1     5.0     snri  effexorxr  \n",
      "6005  0.0  0       0.0       1     5.0     snri  effexorxr  \n",
      "6006  0.0  0       0.0       1     5.0     snri  effexorxr  \n",
      "6007  0.0  0       0.0       0     5.0     snri  effexorxr  \n",
      "6008  0.0  0       0.0       0     5.0     snri  effexorxr  \n",
      "\n",
      "[6009 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "## Vectorize data into TF-IDF and preprocess data\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text=str(text)\n",
    "    tokens=nltk.word_tokenize(text.lower())\n",
    "    tokens_clean=[t for t in tokens if (t not in stop_words) and (t not in punctuations)]\n",
    "    return ' '.join(tokens_clean)\n",
    "    \n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "punctuations=string.punctuation\n",
    "\n",
    "df=data.parse('Sentence_Labeling')\n",
    "df.drop(df.tail(1).index,inplace=True)\n",
    "df['drug_id']=df['drug_id'].str.lower()\n",
    "df['drug_name']=df['drug_id'].str.replace(r'\\.\\d+','',regex=True)\n",
    "df['sentences']=df['sentences'].apply(preprocess)\n",
    "df.fillna(0,inplace=True)\n",
    "unique_drug_count=df['drug_name'].nunique()\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "tfidfSentences=tfidf.fit_transform(df['sentences'])\n",
    "\n",
    "print(tfidfSentences)\n",
    "\n",
    "print('Number of Drug Types:',unique_drug_count)\n",
    "print(df)\n",
    "\n",
    "## ADR: adverse drug reaction\n",
    "## WD: withdrawal symptom\n",
    "## EF: effective\n",
    "## INF: ineffective\n",
    "## SSI: Sign/symptom/illness - if report contains explicit SSI that patient experienced that are not a result of the drug\n",
    "## DI: drug indication - shows SSI that explicitly mentioned as being resolved because of drug consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\julev\\AppData\\Local\\Temp\\ipykernel_26664\\1541660150.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  newDf['DI'].fillna(0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "## This is just the plain sentences -- Task 1 Feature 1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df['DI'] = pd.to_numeric(df['DI'], errors='coerce')\n",
    "\n",
    "newDf=df\n",
    "newDf['DI'].fillna(0, inplace=True)\n",
    "newDf.drop(columns=[\"id\", \"comment_id\", \"drug_id\", \"sentence_index\", \"Findings\", \"others\", \"rating\", \"category\",\"drug_name\"], axis=1, inplace=True)\n",
    "\n",
    "# x = df['sentences'].values\n",
    "# y = df[['ADR', 'WD', 'EF', 'INF', 'SSI', 'DI']].values\n",
    "# y = np.nan_to_num(y, nan=0).astype(int)\n",
    "\n",
    "# Split data\n",
    "df_train, df_temp = train_test_split(newDf, random_state=42, test_size=0.55, shuffle=True)\n",
    "df_valid, df_test = train_test_split(df_temp, random_state=42, test_size=(1/11), shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length= 6009\n",
      "%len of training set= 44.99916791479448\n",
      "%len of validation set= 49.99167914794475\n",
      "%len of test set= 5.009152937260775\n"
     ]
    }
   ],
   "source": [
    "print('total length=',len(df))\n",
    "print('%len of training set=',len(df_train)/len(df)*100)\n",
    "print('%len of validation set=',len(df_valid)/len(df)*100)\n",
    "print('%len of test set=',len(df_test)/len(df)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This one uses the drug_name with the sentences -- Task 1 Feature 2\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df['DI'] = pd.to_numeric(df['DI'], errors='coerce')\n",
    "# newDf=df\n",
    "# newDf['DI'].fillna(0, inplace=True)\n",
    "# newDf['drug_sentences'] = df['drug_name'] + ' ' + df['sentences']\n",
    "# newDf.drop(columns=[\"id\", \"comment_id\", \"drug_id\", \"sentence_index\", \"Findings\", \"others\", \"rating\", \"category\",\"drug_name\",\"sentences\"], axis=1, inplace=True)\n",
    "# # x = df['drug_sentences'].values\n",
    "# # y = df[['ADR', 'WD', 'EF', 'INF', 'SSI', 'DI']].values\n",
    "# # y = np.nan_to_num(y, nan=0).astype(int)\n",
    "\n",
    "# # Split data\n",
    "# df_train, df_temp = train_test_split(newDf, random_state=42, test_size=0.55, shuffle=True)\n",
    "# df_valid, df_test = train_test_split(df_temp, random_state=42, test_size=(1/11), shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentences  ADR   WD   EF  INF  \\\n",
      "0     extreme weight gain short-term memory loss hai...  1.0  0.0  0.0  0.0   \n",
      "1                                      detoxing lexapro  0.0  0.0  0.0  0.0   \n",
      "2     slowly cut dosage several months took vitamin ...  0.0  0.0  0.0  0.0   \n",
      "3                          10 days completely omg rough  0.0  0.0  0.0  0.0   \n",
      "4     flu-like symptoms dizziness major mood swings ...  0.0  1.0  0.0  0.0   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "6004                                increase dosage yet  0.0  0.0  0.0  0.0   \n",
      "6005           'm hoping able stay 75 mgs long possible  0.0  0.0  0.0  0.0   \n",
      "6006  reading withdrawals little scarey like said pe...  0.0  0.0  0.0  0.0   \n",
      "6007  effexor made huge difference life come experie...  0.0  0.0  1.0  0.0   \n",
      "6008              would small price pay able enjoy life  0.0  0.0  1.0  0.0   \n",
      "\n",
      "      SSI   DI  \n",
      "0     0.0  0.0  \n",
      "1     0.0  0.0  \n",
      "2     0.0  0.0  \n",
      "3     0.0  0.0  \n",
      "4     0.0  0.0  \n",
      "...   ...  ...  \n",
      "6004  0.0  0.0  \n",
      "6005  0.0  0.0  \n",
      "6006  0.0  0.0  \n",
      "6007  0.0  0.0  \n",
      "6008  0.0  0.0  \n",
      "\n",
      "[6009 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(newDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (2704, 7), Test: (301, 7), Valid: (3004, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {df_train.shape}, Test: {df_test.shape}, Valid: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-05\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train['sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetTask1(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = list(df['sentences'])\n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'title': title\n",
    "        }\n",
    "     \n",
    "     \n",
    "class CustomDatasetTask2(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = list(df['drug_sentences'])\n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'title': title\n",
    "        }\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ADR', 'WD', 'EF', 'INF', 'SSI', 'DI']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list = list(['ADR', 'WD', 'EF', 'INF', 'SSI', 'DI'])\n",
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change this to CustomDatasetTask_ for feature _\n",
    "train_dataset = CustomDatasetTask1(df_train, tokenizer, MAX_LEN, target_list)\n",
    "valid_dataset = CustomDatasetTask1(df_valid, tokenizer, MAX_LEN, target_list)\n",
    "test_dataset = CustomDatasetTask1(df_test, tokenizer, MAX_LEN, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  3635,  5114, 22356,  2606,  3279,  3637, 16915,   102,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'targets': tensor([1., 0., 0., 0., 0., 0.]),\n",
       " 'title': 'weight gain agitation hair loss sleep disturbance'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, 6)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "\n",
    "# # Freezing BERT layers: (tested, weaker convergence)\n",
    "# for param in model.bert_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_loader, model, optimizer):\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to training mode (activate droput, batch norm)\n",
    "    model.train()\n",
    "    # initialize the progress bar\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), \n",
    "                      leave=True, colour='steelblue')\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(ids, mask, token_type_ids) # (batch,predict)=(32,8)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n",
    "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        correct_predictions += np.sum(outputs==targets)\n",
    "        num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        #loop.set_description(f\"\")\n",
    "        #loop.set_postfix(batch_loss=loss)\n",
    "\n",
    "    # returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to eval mode (turn off dropout, fix batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            correct_predictions += np.sum(outputs==targets)\n",
    "            num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "model_save_path = './model2_task1_feature1_model(unfrozen)'\n",
    "start_time = time.time()\n",
    "\n",
    "# for epoch in range(1, EPOCHS+1):\n",
    "#     print(f'Epoch {epoch}/{EPOCHS}')\n",
    "#     model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "#     val_acc, val_loss = eval_model(val_data_loader, model, optimizer)\n",
    "\n",
    "#     print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f} train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "#     history['train_acc'].append(train_acc)\n",
    "#     history['train_loss'].append(train_loss)\n",
    "#     history['val_acc'].append(val_acc)\n",
    "#     history['val_loss'].append(val_loss)\n",
    "\n",
    "#     if val_acc > best_accuracy:\n",
    "#         if not os.path.exists(model_save_path):\n",
    "#             os.makedirs(model_save_path)\n",
    "#         torch.save(model.state_dict(), os.path.join(model_save_path, 'model_state.bin'))\n",
    "#         print(f\"Model saved to {model_save_path}\")\n",
    "#         best_accuracy = val_acc\n",
    "         \n",
    "# end_time = time.time()\n",
    "# total_train_time = end_time - start_time\n",
    "\n",
    "# print(f\"Total training time: {total_train_time:.2f} seconds\")\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "# plt.plot(history['train_acc'], label='train accuracy')\n",
    "# plt.plot(history['val_acc'], label='validation accuracy')\n",
    "# plt.title('Training history')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.legend()\n",
    "# plt.ylim([0, 1])\n",
    "# plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained model (best model)\n",
    "model_load_path = './model2_task1_feature1_model(unfrozen)'\n",
    "model = BERTClass()\n",
    "model.load_state_dict(torch.load(os.path.join(model_load_path, 'model_state.bin')))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = eval_model(test_data_loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9839424141749723"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "     \n",
    "def get_predictions(model, data_loader):\n",
    "    \"\"\"\n",
    "    Outputs:\n",
    "      predictions - \n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "    \n",
    "    titles = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    target_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for data in data_loader:\n",
    "        title = data[\"title\"]\n",
    "        ids = data[\"input_ids\"].to(device, dtype = torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype = torch.float)\n",
    "        \n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "        outputs = torch.sigmoid(outputs).detach().cpu()\n",
    "        # thresholding at 0.5\n",
    "        preds = outputs.round()\n",
    "        targets = targets.detach().cpu()\n",
    "\n",
    "        titles.extend(title)\n",
    "        predictions.extend(preds)\n",
    "        prediction_probs.extend(outputs)\n",
    "        target_values.extend(targets)\n",
    "    \n",
    "    predictions = torch.stack(predictions)\n",
    "    prediction_probs = torch.stack(prediction_probs)\n",
    "    target_values = torch.stack(target_values)\n",
    "    \n",
    "    return titles, predictions, prediction_probs, target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, predictions, prediction_probs, target_values = get_predictions(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "titles:301 \n",
      "predictions:torch.Size([301, 6]) \n",
      "prediction_probs:torch.Size([301, 6]) \n",
      "target_values:torch.Size([301, 6])\n"
     ]
    }
   ],
   "source": [
    "print(f\"titles:{len(titles)} \\npredictions:{predictions.shape} \\nprediction_probs:{prediction_probs.shape} \\ntarget_values:{target_values.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ADR       0.95      0.97      0.96       108\n",
      "          WD       0.88      0.83      0.86        18\n",
      "          EF       0.96      0.89      0.92        55\n",
      "         INF       1.00      0.92      0.96        13\n",
      "         SSI       0.92      0.97      0.94        34\n",
      "          DI       1.00      0.89      0.94        27\n",
      "\n",
      "   micro avg       0.95      0.93      0.94       255\n",
      "   macro avg       0.95      0.91      0.93       255\n",
      "weighted avg       0.95      0.93      0.94       255\n",
      " samples avg       0.61      0.60      0.60       255\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_values, predictions, target_names=target_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentences  ADR   WD   EF  INF  \\\n",
      "2018  allow think one subject time previous 9,846 th...  0.0  0.0  1.0  0.0   \n",
      "5762                   since power chair almost nothing  0.0  0.0  0.0  0.0   \n",
      "5715                         dizziness standing bending  1.0  0.0  0.0  0.0   \n",
      "1870  started taking 100mg went 50mg due headaches a...  1.0  0.0  0.0  0.0   \n",
      "5142                            3 weeks stopped working  0.0  0.0  0.0  1.0   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "1894                                             sweats  1.0  0.0  0.0  0.0   \n",
      "2240    zoloft made difference life 'm longer depressed  0.0  0.0  1.0  0.0   \n",
      "2677                               first week two bliss  0.0  0.0  1.0  0.0   \n",
      "4411                  currently take 40 mg cymbalta day  0.0  0.0  0.0  0.0   \n",
      "191                               'm serious pain right  1.0  0.0  0.0  0.0   \n",
      "\n",
      "      SSI   DI  \n",
      "2018  1.0  1.0  \n",
      "5762  0.0  0.0  \n",
      "5715  0.0  0.0  \n",
      "1870  0.0  0.0  \n",
      "5142  0.0  0.0  \n",
      "...   ...  ...  \n",
      "1894  0.0  0.0  \n",
      "2240  1.0  1.0  \n",
      "2677  0.0  0.0  \n",
      "4411  0.0  0.0  \n",
      "191   0.0  0.0  \n",
      "\n",
      "[301 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "true_test_labels = df_test[['ADR', 'WD', 'EF', 'INF', 'SSI', 'DI']].to_numpy(dtype=np.float32)\n",
    "print(true_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix for class 'ADR':\n",
      "[[188   5]\n",
      " [  3 105]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       ADR_0       0.98      0.97      0.98       193\n",
      "       ADR_1       0.95      0.97      0.96       108\n",
      "\n",
      "    accuracy                           0.97       301\n",
      "   macro avg       0.97      0.97      0.97       301\n",
      "weighted avg       0.97      0.97      0.97       301\n",
      "\n",
      "Confusion matrix for class 'WD':\n",
      "[[281   2]\n",
      " [  3  15]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        WD_0       0.99      0.99      0.99       283\n",
      "        WD_1       0.88      0.83      0.86        18\n",
      "\n",
      "    accuracy                           0.98       301\n",
      "   macro avg       0.94      0.91      0.92       301\n",
      "weighted avg       0.98      0.98      0.98       301\n",
      "\n",
      "Confusion matrix for class 'EF':\n",
      "[[244   2]\n",
      " [  6  49]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        EF_0       0.98      0.99      0.98       246\n",
      "        EF_1       0.96      0.89      0.92        55\n",
      "\n",
      "    accuracy                           0.97       301\n",
      "   macro avg       0.97      0.94      0.95       301\n",
      "weighted avg       0.97      0.97      0.97       301\n",
      "\n",
      "Confusion matrix for class 'INF':\n",
      "[[288   0]\n",
      " [  1  12]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       INF_0       1.00      1.00      1.00       288\n",
      "       INF_1       1.00      0.92      0.96        13\n",
      "\n",
      "    accuracy                           1.00       301\n",
      "   macro avg       1.00      0.96      0.98       301\n",
      "weighted avg       1.00      1.00      1.00       301\n",
      "\n",
      "Confusion matrix for class 'SSI':\n",
      "[[264   3]\n",
      " [  1  33]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       SSI_0       1.00      0.99      0.99       267\n",
      "       SSI_1       0.92      0.97      0.94        34\n",
      "\n",
      "    accuracy                           0.99       301\n",
      "   macro avg       0.96      0.98      0.97       301\n",
      "weighted avg       0.99      0.99      0.99       301\n",
      "\n",
      "Confusion matrix for class 'DI':\n",
      "[[274   0]\n",
      " [  3  24]]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        DI_0       0.99      1.00      0.99       274\n",
      "        DI_1       1.00      0.89      0.94        27\n",
      "\n",
      "    accuracy                           0.99       301\n",
      "   macro avg       0.99      0.94      0.97       301\n",
      "weighted avg       0.99      0.99      0.99       301\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion_matrixes=multilabel_confusion_matrix(true_test_labels,predictions.numpy())\n",
    "class_names = ['ADR', 'WD', 'EF', 'INF', 'SSI', 'DI']\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    cm = confusion_matrixes[idx]\n",
    "    print(f\"Confusion matrix for class '{class_name}':\\n{cm}\\n\")\n",
    "    report = classification_report(true_test_labels[:, idx], predictions[:, idx], target_names=[f'{class_name}_0', f'{class_name}_1'])\n",
    "    print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
