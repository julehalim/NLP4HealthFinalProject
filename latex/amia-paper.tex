\documentclass[10.7pt, onecolumn]{article}

\usepackage[letterpaper, margin=2.54cm, top=2.54cm]{geometry}
\usepackage[super,comma,sort&compress]{natbib}
\usepackage{lmodern}
\usepackage{authblk} % To add affiliations to authors
\usepackage{amssymb,amsmath}
\usepackage{wrapfig}
\usepackage{graphicx,grffile}
\usepackage[labelfont=bf,labelsep=period]{caption}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\usepackage{graphicx}
\usepackage{pdfpages}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
    \setmainfont[]{Times New Roman}
    \setsansfont[]{Century Gothic}
    \setmonofont[Mapping=tex-ansi]{Consolas}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
	\usepackage{microtype}
	\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}

\usepackage{float}
\usepackage{lipsum}
\newtheorem{exm}{Example}


%==============================
% Customization to make the output PDF 
% look similar to the MS Word version
%==============================
% To prevent hyphenation
\hyphenpenalty=10000
\exhyphenpenalty=10000

% To set the sections font size
\usepackage{sectsty}
\allsectionsfont{\fontsize{10}{10}\selectfont}
%\sectionfont{\fontsize{10}{10}\selectfont}
\subsectionfont{\itshape\bfseries\fontsize{10}{10}\selectfont}
\subsubsectionfont{\normalfont\itshape}

% No new line after subsubsection
\makeatletter
\renewcommand\subsubsection{\@startsection{subsubsection}{3}{\z@}%
	{-3.25ex\@plus -1ex \@minus -.2ex}%
    {-1.5ex \@plus -.2ex}% Formerly 1.5ex \@plus .2ex
    {\normalfont\itshape}}
\makeatother

\makeatletter % Reference list option change
\renewcommand\@biblabel[1]{#1.} % from [1] to 1
\makeatother %

% To set the doc title font
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@maketitle}{\LARGE}{\bfseries\fontsize{15}{16}\selectfont}{}{}
\makeatother

% No page numbering
\pagenumbering{gobble}

\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother

% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
%==============================
\usepackage{hyperref}
\hypersetup{
	unicode=true,
	pdftitle={NLP4Heatlh_Assignment3_Halim},
	pdfauthor={Jule Valendo Halim - 1425567},
	pdfkeywords={keyword1, keyword2},
	pdfborder={0 0 0},
	breaklinks=true
}
\urlstyle{same}  % don't use monospace font for urls
%==============================

% reduce space between title and begining of page
\title{\vspace{-2em} Investigating the Use of Statistical Tests in Natural Language Processing Machine Learning Models}
\author[ ]{\bf\fontsize{13}{14}\selectfont Jule Valendo Halim -1425567\vspace{-.7em}}
\affil[ ]{\bf\fontsize{13}{14}\selectfont University of Melbourne}
\date{} % add no date (by default date is added)

%==============================
\begin{document}
\maketitle
\vspace{-4em} %separation between the affiliations and abstract
%==============================

%==============================
\section{Abstract}\label{abstract}
%==============================
\subsection{Objective}

To investigate possible roles of statistical tests on natural language processing tasks through models for multi-label and multi-class classifiers.
\subsection{Materials}

Dataset containing online patient reviews of prescribed medication, provided by Zoolnori et al\cite{psyTAR1}\cite{psyTar2}. The dataset contains annotated labels for each review sentence, along with the medication being reviewed.
\subsection{Methods}

Various multi-label and multi-class classification tasks were created. Afterwards, each model was evaluated using traditional machine learning metrics and statistical tests.

\subsection{Results}

When comparing traditional machine learning metrics, transformer models generally outperform their logistic regression, except for one task-feature pair. Statistical test results were reported and interpretations were provided.

\subsection{Discussion}

Interpretations of statistical tests were discussed in the context of model behaviour. Possible limitations and issues were also described.

\subsection{Conclusion}

Statistical tests provide valuable insight into model behaviour outside of traditional machine learning metrics. However, additional work is needed in creating a pipeline to investigate causes of possible model behaviour, as well as integration of assumption testing.

%==============================
\section{Introduction}\label{introduction}
%==============================
Statistical tests are a popular and long-standing method in multiple fields of science. However, studies in natural language processing (NLP) and machine learning (ML) do not generally include statistical testing as part of their model evaluation. A survey on 233 published papers in the field of NLP showed that 132 of these papers did not report statistical significance\cite{statsPaper}. However, more studies have begun advocating for the use of these tests to show that experimental results are not coincidental\cite{statsPaper} and argue that a combination of NLP and statistical tests can provide a framework for the development of robust, high-throughput health NLP systems\cite{10.1197/jamia.M3028}. 

In this report, I aim to investigate the use of statistical tests on ML models that predict multi-label and multi-class  classification tasks using the statistical test workflow suggested by Rainio, Tauho, and Klén\cite{statsBased}, shown in figure \ref{fig:statistical test workflow}, and compare different feature inputs.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/41598_2024_56706_Fig3_HTML.png}
  \caption{Statistical Workflow Provided by Rainio, Tauho, and Klén\cite{statsBased}}
  \label{fig:statistical test workflow}
\end{figure}
 
The Sentence\_Labeling sheet of the PsyTAR dataset used in this study is provided by Zoolnoori et al\cite{psyTAR1}\cite{psyTar2}. This sheet contains three sections of interest. The first is sentences from online patient reviews of certain drugs. The second is annotations of certain labels, described below in table \ref{tab:6Labels}. The third is drug labels, which indicates which drug each sentence is reviewing. Figure \ref{tab:stats} shows descriptive statistics of the dataset.

Two classification tasks were performed using a baseline logistic regression model and a transformer model. The multi label class predicts six binary annotations, while the multi-class predicts four different drugs. For each model, different features will be used as inputs. The resulting predictions will be used for statistical tests to determine their statistical significance. 

\begin{table}[h!]
  \centering
  \begin{tabular}{|l|l|r|}
      \hline
      \multicolumn{2}{|l|}{\textbf{Number of Sentences}} & 6009 \\
      \hline
      \multicolumn{2}{|l|}{\textbf{Sample Review Text}} & \parbox{10cm}{extreme weight gain, short-term memory loss, hair loss.} \\
      \hline
      \multicolumn{2}{|l|}{\textbf{Drug Class Distribution}} & \\
      \hline
      & Cymbalta & 1707 \\
      \cline{2-3}
      & Lexapro & 1492 \\
      \cline{2-3}
      & Effexorxr & 1549 \\
      \cline{2-3}
      & Zoloft & 1261 \\
      \hline
      \multicolumn{2}{|l|}{\textbf{Review Text Label (Positive)}} & \\
      \hline
      & ADR (Positive) & 2169 \\
      \cline{2-3}
      & WD (Positive) & 439 \\
      \cline{2-3}
      & EF (Positive) & 1088 \\
      \cline{2-3}
      & INF (Positive) & 338 \\
      \cline{2-3}
      & SSI (Positive) & 790 \\
      \cline{2-3}
      & DI (Positive) & 622 \\
      \hline
      \multicolumn{2}{|l|}{\textbf{Review Text Label (Negative)}} & \\
      \hline
      & ADR (Negative) & 3840 \\
      \cline{2-3}
      & WD (Negative) & 5570 \\
      \cline{2-3}
      & EF (Negative) & 4921 \\
      \cline{2-3}
      & INF (Negative) & 5671 \\
      \cline{2-3}
      & SSI (Negative) & 5219 \\
      \cline{2-3}
      & DI (Negative) & 5387 \\
      \hline
  \end{tabular}
  \caption{Descriptive Statistics from Sentence Labeling Dataset - Description of Review Text Labels Are Found in Table \ref{tab:6Labels}.}
  \label{tab:stats}
\end{table}
%==============================
\section{Methods}\label{methods}
%==============================
% ASK ON WHETHER I NEED TO DESCRIBE IN DETAIL THE STATISTCAL TESTS USED

\subsection{Preprocessing}
Two main preprocessing steps were done. Firstly, as the drug labels in the data were concatenated with review ID, the review ID was stripped and only the drug label was added to a new column. Secondly, the review text was preprocessed through tokenization, as well as stopword and punctuation removal. The data was split into train, validation, and test sets using a 45:45:10 ratio. Additional feature processing specific to each task is described in their respective sections.
\subsection{Multi-Label Classification}
Multi-label classification involves predicting six labels, as shown in table \ref{tab:6Labels}. Each label is a binary task (1 for present and 0 for absent), which has been manually annotated. This task aims to predict annotations for a given review text.

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{|c|c|}
    \hline
    \textbf{Predicted Class} & \textbf{Description} \\
    \hline
    Adverse Drug Reactions (ADR) & Adverse reactions to the drug \\
    \hline
    Withdrawal Symptom (WD) & Withdrawal symptoms after they stopped using the drug \\
    \hline
    Effective (EF) & Drug is effective \\
    \hline
    Ineffective (INF) & Drug is ineffective\\
    \hline
    Sign/Symptom/Illness (SSI) & Text contains explicit SSI as a result of the drug \\
    \hline
    Drug Indication (DI) & Text contains SSI that is currently being addressed by the drug \\
    \hline
  \end{tabular}
  \caption{The Six Labels Predicted by Multi-Label Classifiers}
  \label{tab:6Labels}
\end{table}

\subsubsection{Selected Features}

Two features were used for multi-label classification. The first is the preprocessed review texts without any additional features. From here on, references to this task-feature pair will be referred as task 1 feature 1. The second contains the drug name added to the start of the preprocessed review text. This will be referred to as task 1 feature 2.

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{|p{5cm}|p{5cm}|p{5cm}|}
    \hline
    \textbf{Feature 1} & \textbf{Feature 2} \\
    \hline
    extreme weight gain short-term memory hair loss & lexapro extreme weight gain short-term memory hair loss\\
    \hline
  \end{tabular}
  \caption{Sample Inputs for Each Feature in Multi-Label Classification}
  \label{tab:multiclassInput}
\end{table}

\subsubsection{Logistic Regression}
The logistic regression model performed Term Frequency Inverse Document Frequency (TF-IDF) vectorization on the input text. A multi-output classifier was built upon a logistic regression model, which was then tuned using hyperparameter tuning.

\subsubsection{Transformer}
The transformer model uses a pre-trained BERT model, which was trained on a downstream task in order to create a multi-label transformer classifier. Binary cross-entropy loss (BCE) with logits (a sigmoid layer) loss was used for loss calculation. BCE is well-suited for binary classification\cite{xu2023learning}. 

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{|p{4cm}|p{4cm}|}
    \hline
    \textbf{Parameter} & \textbf{Value} \\
    \hline
    hidden\_size & 768 \\
    \hline
    num\_hidden\_layers & 24 \\
    \hline
    num\_attention\_heads & 12 \\
    \hline
    intermediate\_size & 3072 \\
    \hline
    num\_labels & 6 \\
    \hline
    optimizer & AdamW \\
    \hline
    loss\_calculation & BCE With Logits Loss \\ 
    \hline
    epochs & 15 \\
    \hline
  \end{tabular}
  \caption{BERT Configuration for Multi-Label Classification}
  \label{tab:task1Parameters}
\end{table}

\subsubsection{Statistical Tests}

Multi-label classification will be tested using macro averaged precision, recall, and F1-scores. The choice to use macro instead of micro was to give equal weights to each class, as some labels were more prevalent than others. Accuracy will also be reported.

The Hamming Loss (HL) of the predictions are also calculated. HL measures the fraction of labels that are incorrectly predicted, on average, across all samples and is used to evaluate multi-label classification tasks\cite{hammingloss}. HL ranges from 0 to 1, where 1 means all predictions are erroneous, while 0 means perfect predictions. Appendix A contains statistical test equations.

\subsection{Multi-Class Classification}

Multiclass classification aims to take in varying inputs and predict the drug that is being reviewed. There are four classes of drugs to predict; lexapro, cymbalta, effexorxr, and zoloft.

\subsubsection{Selected Features}

Three features were selected for multi-class classification. The first is the preprocessed sentence without any additional features, referred to as task 2 feature 1. 

The second is the preprocessed text along with its annotations. For the logistic regression model, the binary annotations are simply added onto the end of the sentences as 1s and 0s. 

The transformer's input has the review text concatenated with the predicted class names. If the class is labelled 1, a [POS] token was placed in front of it. If the class is labelled 0, a [NEG] token was placed instead. These tokens identify positive(1) and negative(0) labels respectively. This feature will be referred to as task 2 feature 2.

The third feature is to only use the annotation inputs. This will be referred to as task 2 feature 3. The logistic regression model takes in only the binary annotations, while the transformer only takes in the predicted class name along with the described tokens.

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{|p{5cm}|p{5cm}|p{5cm}|}
    \hline
    \textbf{Feature 1 Transformer} & \textbf{Feature 2 Transformer} & \textbf{Feature 3 Transformer} \\
    \hline
    extreme weight gain short-term memory hair loss & [POS] adverse drug reaction [NEG] withdrawal symptoms...extreme weight gain short-term memory hair loss &[POS] adverse drug reaction [NEG] withdrawal symptoms...[NEG] drug indication\\
    \hline
    \textbf{Feature 1 Logistic Regression} & \textbf{Feature 2 Logistic Regression} & \textbf{Feature 3 Logistic Regression} \\
    \hline
    extreme weight gain short-term memory hair loss & extreme weight gain short-term memory hair loss 1 0 1 1 0 0 & 1 0 1 1 0 0\\
    \hline
  \end{tabular}
  \caption{Sample Inputs for Each Feature in Multi-Class Classification}
  \label{tab:multiclassInput}
\end{table}

\subsubsection{Logistic Regression}

TF-IDF vectorization was used on the input text. However, in contrast to using a multi-label classifier built on top of a logistic regression model, multi-class classification uses logistic regression directly.

\subsubsection{Transformer}
The transformer model is identical to the multi-label transformer, except it was trained on a multi-class downstream task. The number of hidden layers was also decreased due to long training times. The loss calculation was also changed to cross-entropy as it is a better fit for classification tasks\cite{hui2021evaluation}.

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{|p{4cm}|p{3cm}|}
    \hline
    \textbf{Parameter} & \textbf{Value} \\
    \hline
    hidden\_size & 768 \\
    \hline
    num\_hidden\_layers & 8 \\
    \hline
    num\_attention\_heads & 12 \\
    \hline
    intermediate\_size & 3072 \\
    \hline
    num\_labels & 1 \\
    \hline
    optimizer & AdamW \\
    \hline
    loss\_calculation & Cross-Entropy Loss \\
    \hline
    epochs & 20 \\
    \hline
  \end{tabular}
  \caption{BERT Configuration for Multi-Class Classification}
  \label{tab:task2Parameters}
\end{table}
\subsubsection{Statistical Tests}
Multi-class classification will be tested using macro averaged precision, recall, and F1-scores as described previously. The use of macro averaging was due to class imbalance (zoloft had a test count of 565 while cymbalta had a test count of 791). Accuracy will also be reported.

Additionally, following figure \ref{fig:statistical test workflow}, Cohen's Kappa\cite{d4} (Cohen's K) and Matthews Correlation Coefficient (MCC) will be used. Cohen's K is used to calculate inter-model agreement on predictions. It returns a value between 1 and -1, where 1 means a perfect agreement, 0 means no agreement above chance, and -1 indicating less agreement than random chance.

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{|p{4cm}|p{3cm}|}
    \hline
    \textbf{Absolute Cohen's Kappa Range} & \textbf{Interpretation} \\
    \hline
    $|\kappa| \leq 0$ & No agreement \\
    \hline
    $0.01 \leq |\kappa| \leq 0.20$ & None to slight agreement \\
    \hline
    $0.21 \leq |\kappa| \leq 0.40$ & Fair agreement \\
    \hline
    $0.41 \leq |\kappa| \leq 0.60$ & Moderate agreement \\
    \hline
    $0.61 \leq |\kappa| \leq 0.80$ & Substantial agreement \\
    \hline
    $0.81 \leq |\kappa| \leq 1.00$ & Almost perfect agreement \\
    \hline
  \end{tabular}
  \caption{Cohen's Kappa Cutoff Points Based on McHugh's Research \cite{cohen}}
  \label{tab:kappaInterpretation}
\end{table}

MCC has been adapted for multi-class classification by considering all the true and false positive as well as true and false negatives for each class\cite{mccmulticlass}. This adaptation of the MCC can indicate whether model performance across all classes.

MCC returns a value between 1 and -1, where 1 means perfect predictions, 0 means a prediction that is no better than random, and -1 means total disagreement between predictions and ground truth. MCC follows cutoff point selection of graphs such as an area under a reciever operating characteristic (AUROC) curve\cite{articlesss}\cite{Yang_Berdine_2017}. However, this report will follow arbitrary cutoff points.

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{|p{4cm}|p{3cm}|}
    \hline
    \textbf{Absolute MCC Values} & \textbf{Interpretation} \\
    \hline
    $0 \leq |\text{MCC}| \leq 0.1$ & Very Poor Predictions \\
    \hline
    $0.1 < |\text{MCC}| < 0.3$ & Poor Predictions \\
    \hline
    $0.3 \leq |\text{MCC}| < 0.5$ & Moderate Predictions \\
    \hline
    $|\text{MCC}| \geq 0.7$ & Good Predictions \\
    \hline
  \end{tabular}
  \caption{MCC Cutoff Points}
  \label{tab:mccInterpretation}
\end{table}

\subsection{Friedman's Test of Significance}
In order to investigate whether the predictions are significantly different from each other, Friedman's test will be attempted to be performed on both tasks. Friedman's test\cite{7c3c84e5-7230-3033-8b6c-ec430fb73d61}\cite{milton} is a hypothesis testing method, where the null hypothesis is that there is no significant difference between two samples of predictions\cite{HOFFMAN2015421}. Meanwhile, the alternative hypothesis is that a significant difference does exist. A cutoff point ($\alpha$ value) of 0.05 will be used, meaning that if the p-value is <0.05, the null hypothesis will be rejected.

This test was found to not be suitable for the multi-class classification task due to the output. Additional discussion on the impact of this will be discussed in the discussion and conclusion section. For multi-label classification, each label had the Friedman's test performed.

\begin{itemize}
  \item \textbf{Null Hypothesis (\(H_0\))}: There is no significant difference between the models' predictions for the label.

  \item \textbf{Alternative Hypothesis (\(H_1\))}: There is a significant difference between the models' predictions for the label.

  \item \( \alpha = 0.05 \)
\end{itemize}

\subsection{Ethics Statement}

One ethical consideration for this study is that interpretations of statistical test results should only be seen as possible recommendations and indicators of possible model behaviour. As such, additional testing and observations need to be done to confirm these interpretations.

Secondly, this study proposes some potential issues regarding the statistical workflow by Rainio, Tauho, and Klén\cite{statsBased}. However, this should not be seen as a direct criticism of their study. Rather, it serves to highlight possible difficulties in the interpretations of how to apply statistical tests to natural language processing models.

Finally, this study proposes changes to the way machine learning models are evaluated by the wider scientific community. However, these are proposed changes require significantly more robust investigation into their advantages before being taken as the norm for evaluating machine learning tasks. Instead, this study serves as an investigation into current methods and pipelines suggested by other studies in the field of combining statistical tests with traditional machine learning metrics.

%==============================
\section{Results and Analysis}\label{results}
%==============================

\subsection{Multi-Label Statistical Tests}

\subsubsection{Accuracy, Precision, Recall, and F1-Scores}
Table \ref{tab:task1accEtc} shows the accuracy along with the macro precision, recall, and F1 scores. Transformer models also show training and validation loss.
\begin{table}[h]
  \small
  \centering
  \setlength{\tabcolsep}{4pt}
  \begin{tabular}{|p{3cm}|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} & \textbf{Macro Precision} & \textbf{Macro Recall} & \textbf{Macro F1}  & \textbf{Training Loss} & \textbf{Validation Loss}\\
    \hline
    Task 1 Feature 1 Transformer & 0.98  & 0.95  & 0.91  & 0.93 & 0.0134 & 0.0789 \\
    \hline
    Task 1 Feature 1 Logistic Regression & 0.5083 & 0.64  & 0.38  & 0.47 &N/A &N/A \\
    \hline
    Task 1 Feature 2 Transformer & 0.98  & 0.92  & 0.93  & 0.92 &0.0115 &0.0772 \\
    \hline
    Task 1 Feature 2 Logistic Regression & 0.5249 & 0.65  & 0.38  & 0.47 &N/A &N/A \\
    \hline
  \end{tabular}
  \caption{Accuracy and Macro-Averaged Precision, Recall, F1-Scores, Training Loss, and Validation Loss for Task 1}
  \label{tab:task1accEtc}
\end{table}

Transformers show much higher performances compared to logistic regression for both features. Change in features does not appear to significantly affect model performance for all models, with the highest change being accuracy in logistic regression models (1.66\% difference).

\subsubsection{Hamming Loss}

The results of calculating HL for multi-label classification are shown in table \ref{tab:hammingL}. Each model was compared to the ground truth.

\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{|c|c|}
      \hline
      \textbf{Model} & \textbf{Hamming Loss Against Ground Truth} \\ \hline
      Task 1 Feature 1 Transformer & 0.016057586 \\ \hline
      Task 1 Feature 1 Logistic Regression      & 0.11517165  \\ \hline
      Task 1 Feature 2 Transformer & 0.019379845 \\ \hline
      Task 1 Feature 2 Logistic Regression      & 0.109634551 \\ \hline
  \end{tabular}
  \caption{Hamming Loss for Multi-Label Task (Task 1)}
  \label{tab:hammingL}
\end{table}

The transformer models have significantly lower HL than the logistic regression models. This is as expected, as a higher accuracy indicates a higher chance of correct predictions. However, HL can show a more detailed view on model performance. For example, feature 1 using logistic regression has an accuracy of 50.83\%. This accuracy metric is strict as it only considers an instance as correct if all labels were correctly predicted. However, the HL for this model suggests that only about 11.16\% of the predictions were incorrect, indicating that the model performs better when considering individual label predictions.

\subsubsection{Friedman's Test of Significance}
%% write some stuff that connects all three. Accuracy etc etc provides a high-level overview, while HL can show increased detailed performance. Friedmans shows that these predictions are significant, and that they are not due to random chance.
Friedman's test was done on all the models simultaneously. It returns two values, a test statistic and a p-value. Results are shown in table \ref{tab:friedmansTest}.

\begin{table}[H]
  \centering
  \small
  \begin{tabular}{|c|c|c|}
  \hline
  \textbf{Label} & \textbf{Statistic} & \textbf{p-value} \\ 
  \hline
  ADR & 16.373 & 0.003 \\ 
  \hline
  WD & 10.667 & 0.031 \\ 
  \hline
  EF & 64.681 & $3.00 \times 10^{-13}$ \\ 
  \hline
  INF & 12.653 & 0.013 \\ \hline
  SSI & 34.712 & $5.32 \times 10^{-7}$ \\ 
  \hline
  DI & 27.855 & $1.33 \times 10^{-5}$ \\ 
  \hline
  \end{tabular}
  \caption{Friedman's Test of Significance for Multi-Label Task (Task 1)}
  \label{tab:friedmansTest}
\end{table}

The results of Friedman's test statistic shows the difference in predictions for all models. These differences are statistically significant, as each of the p-values are less than the determined cutoff point (0.05). As such, we reject the null hypothesis. These results provide evidence in favor of the alternative hypothesis that each model's predictions are significantly different.

% Statistic is how different it is (higher stat=higher difference). THe p-value shows that these differences are significant

\subsection{Multi-Class Statistical Tests}

\subsubsection{Accuracy, Precision, Recall, and F1-Scores}
Table \ref{tab:task2accEtc} shows the accuracy along with the macro precision, recall, and F1 scores. Transformer models also show training and validation loss.
\begin{table}[H]
  \small
  \centering
  \setlength{\tabcolsep}{4pt} 
  \begin{tabular}{|p{3cm}|c|c|c|c|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} & \textbf{Macro Precision} & \textbf{Macro Recall} & \textbf{Macro F1}  & \textbf{Training Loss} & \textbf{Validation Loss}\\
    \hline
    Task 2 Feature 1 Transformer & 0.84  & 0.84  & 0.84  & 0.84 &0.044 &0.286\\
    \hline
    Task 2 Feature 1 Logistic Regression & 0.3688 & 0.78  & 0.35  & 0.29 & N/A &N/A\\
    \hline
    Task 2 Feature 2 Transformer & 0.834 & 0.84  & 0.84  & 0.83 &0.031 &0.289 \\
    \hline
    Task 2 Feature 2 Logistic Regression & 0.3688 & 0.78  & 0.35  & 0.29 &N/A &N/A\\
    \hline
    Task 2 Feature 3 Transformer & 0.322 & 0.33  & 0.31  & 0.26 &0.696 &0.696\\
    \hline
    Task 2 Feature 3 Logistic Regression & 0.3422 & 0.28  & 0.31  & 0.24 &N/A &N/A\\
    \hline
  \end{tabular}
  \caption{Accuracy and Macro-Averaged Precision, Recall, F1-Scores, Training Loss, and Validation Loss for Task 2}
  \label{tab:task2accEtc}
\end{table}

The resulting predictions of the model for multi-class classification generally show a high accuracy for transformers, compared to their logistic regression baselines. However, feature 3 showed a different trend, where the logistic regression baseline performed better. There was also a large change in accuracy compared to other transformer models (with feature 3 having roughly 50\% lower accuracy than other transformer models). This could be due to how transformers require rich language information, which feature 3 does not provide, as the inputs are only a series of token-word pairs.
\subsubsection{Cohen's Kappa}
A heatmap of Cohen's K for each model tested against each other is shown in figure \ref{fig:cohenK}.
\begin{figure}[H]
  \centering
  \includegraphics[]{images/CohensK.png}
  \caption{Heatmap of Cohen's Kappa Results on Multi-Class Classification Task}
  \label{fig:cohenK}
\end{figure}

The results of Cohen's K shows that for most models, the agreement between predictions are low. This is true even for the models with high accuracy. For example, the transformer models for features 1 and 2 have similar accuracies (84\% and 83.4\% respectively). However, they have a Cohen's K value of -0.0116. This indicates that on the wrong predictions, these models do not predict the same incorrect class even if they have high accuracy on correct predictions.

Two Cohen's K values are of note. The first is between the logistic regression models of feature 1 and 2, which has a Cohen's K of 0.99, indicating an almost perfect positive agreement. This could be due to how the inputs for feature 1 and 2 are similar, and logistic regression models predict similar patterns for each of these. As such, the models predict similar classes, even for incorrect predictions.

The second is between the transformer models of feature 1 and feature 3 with a Cohen's K of 0.39, indicating substantial positive agreement. Upon inspection, predictions of the transformer models for feature 3 only predict mainly one class. This could cause a higher than expected agreement with the transformer model for feature 1, depending on how the model for feature 1 predicts incorrect predictions.

\subsubsection{Matthews Correlation Coefficient}
Each model was compared against the ground truth to calculate their MCC values. The resulting values are shown in table \ref{tab:mccTab}.

\begin{table}[h!]
  \centering
  \small
  \begin{tabular}{|p{1cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|p{2cm}|}
      \hline
      & \textbf{Task 2 Feature 1 Transformer} & \textbf{Task 2 Feature 1 Logistic Regression} & \textbf{Task 2 Feature 2 Transformer} & \textbf{Task 2 Feature 2 Logistic Regression} & \textbf{Task 2 Feature 3 Transformer} & \textbf{Task 2 Feature 3 Logistic Regression} \\
      \hline
      \textbf{MCC Score} & 0.066509038 & 0.194445905 & -0.043092423 & 0.19418937 & 0.103772985 & 0.124725893 \\
      \hline
  \end{tabular}
  \caption{MCC Scores for Each Model, Compared Against Ground Truth}
  \label{tab:mccTab}
\end{table}

The results of the MCC indicate that most models are not able to correctly predict across all classes evenly. For example, the transformer models for features 1 and 2 have MCC scores that indicate very poor predictions when considering performance across all classes. This could suggest that the model performs significantly worse when considering a class-to-class basis, possibly indicating bias towards a certain class (e.g., defaulting to a majority class for incorrect predictions).

However, for some models that do not have high accuracy, they appear to have higher MCC scores. For example, the transformer for feature 3 has an MCC score of 0.10. This particular transformer model predicted only the majority class, allowing it to minimize false positives and false negatives. As such, although they do not have a good accuracy, they are able to minimize misclassifications, leading to a decent MCC score.

%==============================
\section{Discussion}\label{discussion and conclusion}
%==============================

\subsection{Multi-Label Classification Task}
Interpretation of the statistical tests on the multi-label task can be considered in three stages. First, the traditional machine learning metrics show that transformer models are able to outperform their logistic regression baselines consistently. Secondly, the HL values provide further support that transformers are able to outperform logistic regression models, even when considering individual label predictions. However, the HL shows that logistic regression models perform better on a label-by-label basis than suggested by their accuracy. Finally, Friedman's test shows that the difference in model predictions are statistically significant.


\subsection{Multi-Class Classification Task}

Investigation on Cohen's K supports how logistic regression models learn similar patterns given similar inputs, while transformers might learn different weights with similar inputs. Furthermore, Cohen's K can also provide insights into what incorrect predictions are done. While this report does not investigate this behaviour in depth, the results of Cohen's K provides an indicator of possible model behaviour that could be investigated further.

Finally, the results of the MCC scores show that transformer models do not tend to each class equally, which could support the behaviour of the transformer for feature 1 to default to a majority class. While investigating the exact causes of these MCC scores are out of the scope of this report, MCC scores can provide indications on what the model struggles with, such as evenly predicting every class.

\subsection{Friedman's Test on Multi-Class Classification}

The application of Friedman's test on multi-class classification poses a challenge due to the output, which comes in the form of text. Friedman's test calculates differences between a series of predictions. However, text data such as drug names need to be preprocessed into numerical variables. While some studies have used Friedman's test on textual data by converting it into a ranking task\cite{article1}, there is no consensus on how to handle this change in features. Future work could be done to tackle this issue.

\subsection{Limitations}

One limitation of this study was how the dataset was not investigated using certain assumption tests. For example, logistic regression models needs to meet the assumption of independent errors, which was not tested in this study. However, the most vital limitation of this paper is that interpretations are based on possible model behaviours, not actual model behaviours. Further in-depth investigation of proposed model behaviours could provide a deeper understanding into how statistical tests affect model evaluation. For example, an investigation whether the high Cohen's K value between the transformer models in task 2 feature 1 and feature 3 is due to predictions prioritizing the majority class or some other class imbalance would allow for deeper understanding of model behaviour.

\section{Conclusion and Future Directions}

In conclusion, this report has shown how statistical tests, combined with traditional machine learning metrics can help to better understand model behaviour. While statistical tests indicate model behaviour, traditional machine learning models are still important when considering practicality. For example, for multi-class classification, while the transformer models for feature 1 and 2 do not perform evenly on all classes, their accuracy could still make a transformer model preferred in performing predictions. In addition, while some studies have suggested how MCC has advantages over F1-scores and accuracy\cite{articlesss}, these metrics should still be understood as providing ideas on model behaviour, instead of replacing traditional machine learning metrics.

Future work can investigate more sophisticated workflows to understand model behaviour based on statistical tests. Additional statistical tests could also be integrated, such as testing assumptions of independence, which could guide what statistical tests fit a specific task and to identify possible problems with a provided dataset\cite{article2}.

% future work could include additional statistical tests on features (e.g., chi squared tests of feature indpendence) Efficient and inefficient obviously have some feature dependence, but these are labels. nevertheless, these could be investigated

%a challenge is that NLP tasks generally uses one singular text as the input (e.g.,just the review text). However, we still show how investigating feature engineering on the text could provide deeper insights on how the model performs and increases interpretability.

% could expand into studying the use of this on different datasets because it would provide more statistical power (that the models can generalize outside of datasets)
%==============================
\bibliographystyle{vancouver}
\bibliography{literature}
%==============================
\newpage
\section{Appendix}

\subsection{Appendix A - Equations for Statistical Tests}


\textbf{Cohen's K:}

\begin{equation}
  \kappa = \frac{p_o - p_e}{1 - p_e}
  \end{equation}
  
  where:
  \begin{align*}
  p_o & = \text{relative observed agreement among raters} \\
  p_e & = \text{hypothetical probability of chance agreement}
  \end{align*}
  
  ($p_o$) is calculated as:
  
  \begin{equation}
  p_o = \frac{a + d}{a + b + c + d}
  \end{equation}
  
  ($p_e$) is calculated as:
  
  \begin{align}
    p_e &= \left(\frac{(a + b)}{a + b + c + d} \cdot \frac{(a + c)}{a + b + c + d}\right) \notag \\
    &\quad + \left(\frac{(c + d)}{a + b + c + d} \cdot \frac{(b + d)}{a + b + c + d}\right)
    \end{align}

  where:
  \begin{itemize}
      \item $a$ = Number of times both annotators agreed the sample was positive
      \item $d$ = Number of times both annotators agreed the sample was negative
      \item $b$ = Number of times Annotator A said positive but Annotator B said negative
      \item $c$ = Number of times Annotator A said negative but Annotator B said positive
  \end{itemize}

  \textbf{Matthews Correlation Coefficient:}
  \begin{equation}
    \text{MCC} = \frac{TP \cdot TN - FP \cdot FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}
    \end{equation}

  \begin{flalign*}
    \text{where:} \\
    TP & : \text{True Positive} \\
    FP & : \text{False Positive} \\
    TN & : \text{True Negative} \\
    FN & : \text{False Negative} \\
  \end{flalign*}


\textbf{Hamming Loss:}

\[
\text{HL} = \frac{1}{N \times L} \sum_{l=1}^L \sum_{i=1}^N (Y_{i,l} \neq X_{i,l})
\]

\begin{flalign*}
\text{where:} \\
N & : \text{Number of samples} \\
L & : \text{Number of labels} \\
Y_{i,l} & : \text{True label for } i\text{th sample and } \\ 
& l\text{th label} \\
hat{y}_{ij} & : \text{Predicted label for } i\text{th sample and }\\
& l\text{th label} \\
(Y_{i,l} \neq hat{y}_{ij}) & : \text {1} \text{ if } y_{ij} \neq \hat{y}_{ij} \text{ and } 0 \text{ otherwise}
\end{flalign*}


\textbf{Accuracy, Precision, Recall, and F1-Scores:}
\begin{equation}
  Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
\end{equation}

\begin{equation}
  Precision = \frac{TP}{TP+FP}
\end{equation}

\begin{equation}
  Recall = \frac{TP}{TP+FN}
\end{equation}

\begin{equation}
  F1 = \frac{2*Precision*Recall}{Precision+Recall} = \frac{2*TP}{2*TP+FP+FN}
\end{equation}

\begin{flalign*}
  \text{where:} \\
  TP & : \text{True Positive} \\
  FP & : \text{False Positive} \\
  TN & : \text{True Negative} \\
  FN & : \text{False Negative} \\
\end{flalign*}

\textbf{Friedman's Test:}

\begin{equation}
  T = \frac{12 \sum S_{ij}^2}{jk(j+1)} - 3k(j+1)
\end{equation}

where:
\begin{align*}
T & = \text{test statistic} \\
\sum S_{ij}^2 & = \text{sum of the squared sums of ranks for each prediction set} \\
j & = \text{number of prediction sets} \\
k & = \text{number of instances}
\end{align*}
\newpage
\subsection{Appendix B - Transformer Model Performances}

\begin{figure}[H]
  \centering
  \includegraphics[]{images/model2_task1_feature1_plot.png}
  \caption{Task 1 Feature 1 Model Training (Total Training Time: 2056.51 Seconds)}
  \label{fig:t1f1plot}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[]{images/model2_task1_feature2_plot.png}
  \caption{Task 1 Feature 2 Model Training (Total Training Time: 2037.76 Seconds)}
  \label{fig:t1f2plot}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[]{images/model2_task2_feature1_plot.png}
  \caption{Task 2 Feature 1 Model Training (Total Training Time: 7471.89 Seconds)}
  \label{fig:t2f1plot}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[]{images/model2_task2_feature2_plot.png}
  \caption{Task 2 Feature 2 Model Training (Total Training Time: 6587.32 Seconds)}
  \label{fig:t2f2plot}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[]{images/model2_task2_feature3_plot.png}
  \caption{Task 2 Feature 3 Model Training (Total Training Time: 6590.17 Seconds)}
  \label{fig:t2f3plot}
\end{figure}
\subsection{Appendix C - Source Code}

\includepdf[noautoscale=true, scale=0.75,page=-,fitpaper=true]{pdf_final_code.pdf}
\includepdf[noautoscale=true, scale=0.75,page=-,fitpaper=true]{pdf_final_visualization_code.pdf}
\end{document}
