{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "##Imports\n",
    "import pandas as pd\n",
    "import transformers\n",
    "import openpyxl\n",
    "import numpy\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import sys\n",
    "import tqdm.notebook as tq\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Does augmenting classifiers with structured data help?\n",
    "# Do it for both the baseline classical ML and transformers then compare\n",
    "# Try it with just structured data [0s and 1s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 3060 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet Name: Sample\n",
      "Sheet Name: Sentence_Labeling\n",
      "Sheet Name: ADR_Identified\n",
      "Sheet Name: ADR_Mapped\n",
      "Sheet Name: WD_Identified\n",
      "Sheet Name: WD-Mapped \n",
      "Sheet Name: SSI_Identified\n",
      "Sheet Name: SSI_Mapped\n",
      "Sheet Name: DI_Identified\n",
      "Sheet Name: DI_Mapped\n"
     ]
    }
   ],
   "source": [
    "## Reading in the PSYTar data set and parsing it\n",
    "\n",
    "## Use sentence_labelling sheet\n",
    "\n",
    "fileName=\".\\ONLINE_FORA\\PsyTAR_dataset.xlsx\"\n",
    "data=pd.ExcelFile(fileName)\n",
    "sheets={}\n",
    "for sheet in data.sheet_names:\n",
    "    sheets[sheet]=data.parse(sheet)\n",
    "\n",
    "## Remove the first two sheets (License and read_me)\n",
    "sheets.pop('License',None)\n",
    "sheets.pop('read_me',None)\n",
    "\n",
    "## This will print out the sheet names for the whole excel\n",
    "for sheet in sheets.keys():\n",
    "    print(f\"Sheet Name: {sheet}\")\n",
    "\n",
    "#To access a sheet, perform sheet['Sheet_Name']; e.g., sheets['Sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2183)\t0.3560733570424169\n",
      "  (0, 2858)\t0.5335210557775061\n",
      "  (0, 2980)\t0.33695305203929327\n",
      "  (0, 4715)\t0.3577130194209486\n",
      "  (0, 4193)\t0.36113778160835414\n",
      "  (0, 2071)\t0.2627216248356759\n",
      "  (0, 5156)\t0.2390135798252401\n",
      "  (0, 1829)\t0.30083291559032393\n",
      "  (1, 2765)\t0.4585168729312828\n",
      "  (1, 1351)\t0.8886857021677111\n",
      "  (2, 2263)\t0.2708094866549267\n",
      "  (2, 4586)\t0.4316846093641784\n",
      "  (2, 5071)\t0.44358904667953863\n",
      "  (2, 4827)\t0.2570546169681927\n",
      "  (2, 3078)\t0.23875893121623276\n",
      "  (2, 4151)\t0.3207489298964512\n",
      "  (2, 1492)\t0.29911785906957045\n",
      "  (2, 1201)\t0.35264280744022497\n",
      "  (2, 4287)\t0.3220996041881179\n",
      "  (3, 4012)\t0.534375375933816\n",
      "  (3, 3326)\t0.5729355939201095\n",
      "  (3, 1018)\t0.39488574070665183\n",
      "  (3, 1237)\t0.3054771887014283\n",
      "  (3, 5)\t0.3700495936825698\n",
      "  (4, 4801)\t0.3601630582861626\n",
      "  :\t:\n",
      "  (6006, 3810)\t0.266723700884449\n",
      "  (6006, 278)\t0.30278988091992165\n",
      "  (6006, 4033)\t0.24160419308016043\n",
      "  (6006, 1558)\t0.24160419308016043\n",
      "  (6006, 2821)\t0.21617383387506503\n",
      "  (6006, 3470)\t0.20987895271102364\n",
      "  (6006, 1390)\t0.23609327417139986\n",
      "  (6006, 2098)\t0.1721949331238259\n",
      "  (6006, 2792)\t0.15461917912599263\n",
      "  (6007, 5221)\t0.40690224630890753\n",
      "  (6007, 1388)\t0.36244934695018927\n",
      "  (6007, 2358)\t0.40690224630890753\n",
      "  (6007, 987)\t0.36244934695018927\n",
      "  (6007, 5165)\t0.28822766104713454\n",
      "  (6007, 2896)\t0.29736053264983103\n",
      "  (6007, 1619)\t0.26742602182403663\n",
      "  (6007, 1809)\t0.3151406469723878\n",
      "  (6007, 2777)\t0.25257333627086354\n",
      "  (6008, 3452)\t0.462967378880836\n",
      "  (6008, 4290)\t0.4133719291682648\n",
      "  (6008, 1695)\t0.41770974043639536\n",
      "  (6008, 3642)\t0.462967378880836\n",
      "  (6008, 186)\t0.3151272227530411\n",
      "  (6008, 2777)\t0.24953491233016006\n",
      "  (6008, 5269)\t0.25375538243161966\n",
      "Number of Drug Types: 4\n",
      "          id  comment_id        drug_id  sentence_index  \\\n",
      "0        1.0         1.0      lexapro.1             1.0   \n",
      "1        2.0         1.0      lexapro.1             2.0   \n",
      "2        3.0         1.0      lexapro.1             3.0   \n",
      "3        4.0         1.0      lexapro.1             4.0   \n",
      "4        5.0         1.0      lexapro.1             5.0   \n",
      "...      ...         ...            ...             ...   \n",
      "6004  1545.0       228.0  effexorxr.228            14.0   \n",
      "6005  1546.0       228.0  effexorxr.228            15.0   \n",
      "6006  1547.0       228.0  effexorxr.228            16.0   \n",
      "6007  1548.0       228.0  effexorxr.228            17.0   \n",
      "6008  1549.0       228.0  effexorxr.228            18.0   \n",
      "\n",
      "                                              sentences  ADR   WD   EF  INF  \\\n",
      "0     extreme weight gain short-term memory loss hai...  1.0  0.0  0.0  0.0   \n",
      "1                                      detoxing lexapro  0.0  0.0  0.0  0.0   \n",
      "2     slowly cut dosage several months took vitamin ...  0.0  0.0  0.0  0.0   \n",
      "3                          10 days completely omg rough  0.0  0.0  0.0  0.0   \n",
      "4     flu-like symptoms dizziness major mood swings ...  0.0  1.0  0.0  0.0   \n",
      "...                                                 ...  ...  ...  ...  ...   \n",
      "6004                                increase dosage yet  0.0  0.0  0.0  0.0   \n",
      "6005           'm hoping able stay 75 mgs long possible  0.0  0.0  0.0  0.0   \n",
      "6006  reading withdrawals little scarey like said pe...  0.0  0.0  0.0  0.0   \n",
      "6007  effexor made huge difference life come experie...  0.0  0.0  1.0  0.0   \n",
      "6008              would small price pay able enjoy life  0.0  0.0  1.0  0.0   \n",
      "\n",
      "      SSI DI  Findings  others  rating category  drug_name  \n",
      "0     0.0  0       0.0       0     1.0     ssri    lexapro  \n",
      "1     0.0  0       0.0       0     1.0     ssri    lexapro  \n",
      "2     0.0  0       0.0       1     1.0     ssri    lexapro  \n",
      "3     0.0  0       0.0       1     1.0     ssri    lexapro  \n",
      "4     0.0  0       0.0       0     1.0     ssri    lexapro  \n",
      "...   ... ..       ...     ...     ...      ...        ...  \n",
      "6004  0.0  0       0.0       1     5.0     snri  effexorxr  \n",
      "6005  0.0  0       0.0       1     5.0     snri  effexorxr  \n",
      "6006  0.0  0       0.0       1     5.0     snri  effexorxr  \n",
      "6007  0.0  0       0.0       0     5.0     snri  effexorxr  \n",
      "6008  0.0  0       0.0       0     5.0     snri  effexorxr  \n",
      "\n",
      "[6009 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "## Vectorize data into TF-IDF and preprocess data\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text=str(text)\n",
    "    tokens=nltk.word_tokenize(text.lower())\n",
    "    tokens_clean=[t for t in tokens if (t not in stop_words) and (t not in punctuations)]\n",
    "    return ' '.join(tokens_clean)\n",
    "    \n",
    "stop_words=nltk.corpus.stopwords.words('english')\n",
    "punctuations=string.punctuation\n",
    "\n",
    "df=data.parse('Sentence_Labeling')\n",
    "df.drop(df.tail(1).index,inplace=True)\n",
    "df['drug_id']=df['drug_id'].str.lower()\n",
    "df['drug_name']=df['drug_id'].str.replace(r'\\.\\d+','',regex=True)\n",
    "df['sentences']=df['sentences'].apply(preprocess)\n",
    "df.fillna(0,inplace=True)\n",
    "unique_drug_count=df['drug_name'].nunique()\n",
    "\n",
    "tfidf=TfidfVectorizer()\n",
    "tfidfSentences=tfidf.fit_transform(df['sentences'])\n",
    "\n",
    "print(tfidfSentences)\n",
    "\n",
    "print('Number of Drug Types:',unique_drug_count)\n",
    "print(df)\n",
    "\n",
    "## ADR: adverse drug reaction\n",
    "## WD: withdrawal symptom\n",
    "## EF: effective\n",
    "## INF: ineffective\n",
    "## SSI: Sign/symptom/illness - if report contains explicit SSI that patient experienced that are not a result of the drug\n",
    "## DI: drug indication - shows SSI that explicitly mentioned as being resolved because of drug consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = df['drug_name'].astype('category').cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'cymbalta', 1: 'effexorxr', 2: 'lexapro', 3: 'zoloft'}\n"
     ]
    }
   ],
   "source": [
    "code_to_drug = {code: drug for code, drug in enumerate(category_mapping)}\n",
    "print(code_to_drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_labels(row):\n",
    "    labels = [\n",
    "        '[POS] adverse drug reaction' if row['ADR'] == 1 else '[NEG] adverse drug reaction',\n",
    "        '[POS] withdrawal symptom' if row['WD'] == 1 else '[NEG] withdrawal symptom',\n",
    "        '[POS] effective' if row['EF'] == 1 else '[NEG] effective',\n",
    "        '[POS] ineffective' if row['INF'] == 1 else '[NEG] ineffective',\n",
    "        '[POS] sign/symptom/illness' if row['SSI'] == 1 else '[NEG] sign/symptom/illness',\n",
    "        '[POS] drug indication' if row['DI'] == 1 else '[NEG] drug indication'\n",
    "    ]\n",
    "    return ' '.join(labels)\n",
    "\n",
    "df['drug_name'] = df['drug_name'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #this one is for the combined sentences -- Task 2 Feature 1\n",
    "\n",
    "df_word_label = df\n",
    "\n",
    "df_word_label['transformed_sentences'] = df.apply(lambda row: f\"{row['sentences']} {transform_labels(row)}\", axis=1)\n",
    "#print(df_word_label)\n",
    "\n",
    "## ADR: adverse drug reaction\n",
    "## WD: withdrawal symptom\n",
    "## EF: effective\n",
    "## INF: ineffective\n",
    "## SSI: Sign/symptom/illness - if report contains explicit SSI that patient experienced that are not a result of the drug\n",
    "## DI: drug indication - shows SSI that explicitly mentioned as being resolved because of drug consumption\n",
    "df_word_label.drop(columns=[\"id\", \"comment_id\", \"drug_id\", \"sentence_index\", \"Findings\", \"others\", \"rating\", \"category\",\"sentences\", \"ADR\", \"WD\", \"EF\", \"INF\", \"SSI\", \"DI\"], axis=1, inplace=True)\n",
    "\n",
    "df_word_label = df_word_label[['transformed_sentences', 'drug_name']]\n",
    "\n",
    "df_train, df_test = train_test_split(df_word_label, random_state=42, test_size=0.10, shuffle=True)\n",
    "# split test into test and validation datasets\n",
    "df_test, df_valid = train_test_split(df_word_label, random_state=42, test_size=0.50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## This one is just for the text without transformed sentences -- Task 2 Feature 2\n",
    "# df_word_label=df\n",
    "\n",
    "\n",
    "# # x = df_word_label['sentences'].values\n",
    "# # y = df_word_label['drug_name'].astype('category').cat.codes.values\n",
    "# df_word_label.drop(columns=[\"id\", \"comment_id\", \"drug_id\", \"sentence_index\", \"Findings\", \"others\", \"rating\", \"category\", \"ADR\", \"WD\", \"EF\", \"INF\", \"SSI\", \"DI\"], axis=1, inplace=True)\n",
    "# df_word_label = df_word_label[['sentences', 'drug_name']]\n",
    "\n",
    "# # Split data\n",
    "# df_train, df_test = train_test_split(df_word_label, random_state=42, test_size=0.10, shuffle=True)\n",
    "# # split test into test and validation datasets\n",
    "# df_test, df_valid = train_test_split(df_word_label, random_state=42, test_size=0.50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## This one is just for the annotated dataset -- Task 2 Feature 3\n",
    "\n",
    "# df_word_label = df\n",
    "\n",
    "# df_word_label['transformed_labels'] = df_word_label.apply(transform_labels, axis=1)\n",
    "# print(df_word_label['transformed_labels'])\n",
    "\n",
    "# df_word_label.drop(columns=[\"id\", \"comment_id\", \"drug_id\", \"sentence_index\", \"Findings\", \"others\", \"rating\", \"category\",\"sentences\", \"ADR\", \"WD\", \"EF\", \"INF\", \"SSI\", \"DI\"], axis=1, inplace=True)\n",
    "# df_word_label = df_word_label[['transformed_labels', 'drug_name']]\n",
    "\n",
    "# # x = df_word_label['transformed_labels'].values\n",
    "# # y = df_word_label['drug_name'].astype('category').cat.codes.values\n",
    "\n",
    "# #Split data\n",
    "# df_train, df_test = train_test_split(df_word_label, random_state=42, test_size=0.10, shuffle=True)\n",
    "# # split test into test and validation datasets\n",
    "# df_test, df_valid = train_test_split(df_word_label, random_state=42, test_size=0.50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  transformed_sentences  drug_name\n",
      "0     extreme weight gain short-term memory loss hai...          2\n",
      "1     detoxing lexapro [NEG] adverse drug reaction [...          2\n",
      "2     slowly cut dosage several months took vitamin ...          2\n",
      "3     10 days completely omg rough [NEG] adverse dru...          2\n",
      "4     flu-like symptoms dizziness major mood swings ...          2\n",
      "...                                                 ...        ...\n",
      "6004  increase dosage yet [NEG] adverse drug reactio...          1\n",
      "6005  'm hoping able stay 75 mgs long possible [NEG]...          1\n",
      "6006  reading withdrawals little scarey like said pe...          1\n",
      "6007  effexor made huge difference life come experie...          1\n",
      "6008  would small price pay able enjoy life [NEG] ad...          1\n",
      "\n",
      "[6009 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_word_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (5408, 2), Test: (3004, 2), Valid: (3005, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train: {df_train.shape}, Test: {df_test.shape}, Valid: {df_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "VALID_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "LEARNING_RATE = 1e-05\n",
    "THRESHOLD = 0.5 # threshold for the sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDatasetTask1(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = list(df['transformed_sentences'])\n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'title': title\n",
    "        }\n",
    "        \n",
    "class CustomDatasetTask2(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = list(df['sentences'])\n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'title': title\n",
    "        }\n",
    "        \n",
    "class CustomDatasetTask3(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len, target_list):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.title = list(df['transformed_labels'])\n",
    "        self.targets = self.df[target_list].values\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        title = str(self.title[index])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n",
    "            'targets': torch.FloatTensor(self.targets[index]),\n",
    "            'title': title\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['drug_name']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_list = list(['drug_name'])\n",
    "target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDatasetTask1(df_train, tokenizer, MAX_LEN, target_list)\n",
    "valid_dataset = CustomDatasetTask1(df_valid, tokenizer, MAX_LEN, target_list)\n",
    "test_dataset = CustomDatasetTask1(df_test, tokenizer, MAX_LEN, target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  4318,  2677, 15311,  3806,  1038,  4135,  5844,  3356,  2067,\n",
       "         12336,  2926,  2305, 18856,  3286,  8029,  3110, 14978,  6091,  2791,\n",
       "          1031, 13433,  2015,  1033, 15316,  4319,  4668,  1031, 11265,  2290,\n",
       "          1033, 10534, 25353, 27718,  5358,  1031, 11265,  2290,  1033,  4621,\n",
       "          1031, 11265,  2290,  1033, 20694,  1031, 11265,  2290,  1033,  3696,\n",
       "          1013, 25353, 27718,  5358,  1013,  7355,  1031, 11265,  2290,  1033,\n",
       "          4319, 12407,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'targets': tensor([2.]),\n",
       " 'title': 'dry mouth shaky gas bloating upper back ache especially night clammy feeling headache nervousness [POS] adverse drug reaction [NEG] withdrawal symptom [NEG] effective [NEG] ineffective [NEG] sign/symptom/illness [NEG] drug indication'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "    batch_size=TRAIN_BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n",
    "    batch_size=VALID_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_data_loader = torch.utils.data.DataLoader(test_dataset, \n",
    "    batch_size=TEST_BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BERTClass(\n",
       "  (bert_model): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.linear = torch.nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, input_ids, attn_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids, \n",
    "            attention_mask=attn_mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "        return output\n",
    "\n",
    "model = BERTClass()\n",
    "\n",
    "# # Freezing BERT layers: (tested, weaker convergence)\n",
    "# for param in model.bert_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\julev\\anaconda3\\envs\\cudaEnv\\Lib\\site-packages\\transformers\\optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr = 1e-5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_loader, model, optimizer):\n",
    "\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to training mode (activate droput, batch norm)\n",
    "    model.train()\n",
    "    # initialize the progress bar\n",
    "    loop = tq.tqdm(enumerate(training_loader), total=len(training_loader), \n",
    "                      leave=True, colour='steelblue')\n",
    "    for batch_idx, data in loop:\n",
    "        ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        # forward\n",
    "        outputs = model(ids, mask, token_type_ids) # (batch,predict)=(32,8)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n",
    "        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "        targets = targets.cpu().detach().numpy()\n",
    "        correct_predictions += np.sum(outputs==targets)\n",
    "        num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        # grad descent step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update progress bar\n",
    "        #loop.set_description(f\"\")\n",
    "        #loop.set_postfix(batch_loss=loss)\n",
    "\n",
    "    # returning: trained model, model accuracy, mean loss\n",
    "    return model, float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(validation_loader, model, optimizer):\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    num_samples = 0\n",
    "    # set model to eval mode (turn off dropout, fix batch norm)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data in enumerate(validation_loader, 0):\n",
    "            ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            # validation accuracy\n",
    "            # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n",
    "            targets = targets.cpu().detach().numpy()\n",
    "            correct_predictions += np.sum(outputs==targets)\n",
    "            num_samples += targets.size   # total number of elements in the 2D array\n",
    "\n",
    "    return float(correct_predictions)/num_samples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38c90de6e194636b57d212bd6f0f5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/676 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "model_save_path = './model2_task2_feature1_model(unfrozen)'\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n",
    "    val_acc, val_loss = eval_model(val_data_loader, model, optimizer)\n",
    "\n",
    "    print(f'train_loss={train_loss:.4f}, val_loss={val_loss:.4f} train_acc={train_acc:.4f}, val_acc={val_acc:.4f}')\n",
    "\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "\n",
    "    if val_acc > best_accuracy:\n",
    "        if not os.path.exists(model_save_path):\n",
    "            os.makedirs(model_save_path)\n",
    "        torch.save(model.state_dict(), os.path.join(model_save_path, 'model_state.bin'))\n",
    "        print(f\"Model saved to {model_save_path}\")\n",
    "        best_accuracy = val_acc\n",
    "         \n",
    "end_time = time.time()\n",
    "total_train_time = end_time - start_time\n",
    "\n",
    "print(f\"Total training time: {total_train_time:.2f} seconds\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,7)\n",
    "plt.plot(history['train_acc'], label='train accuracy')\n",
    "plt.plot(history['val_acc'], label='validation accuracy')\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1])\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading pretrained model (best model)\n",
    "model_load_path = './model2_task2_feature1_model(unfrozen)'\n",
    "model = BERTClass()\n",
    "model.load_state_dict(torch.load(os.path.join(model_load_path, 'model_state.bin')))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc, test_loss = eval_model(test_data_loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "     \n",
    "def get_predictions(model, data_loader):\n",
    "    \"\"\"\n",
    "    Outputs:\n",
    "      predictions - \n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "    \n",
    "    titles = []\n",
    "    predictions = []\n",
    "    prediction_probs = []\n",
    "    target_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      for data in data_loader:\n",
    "        title = data[\"title\"]\n",
    "        ids = data[\"input_ids\"].to(device, dtype = torch.long)\n",
    "        mask = data[\"attention_mask\"].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data[\"targets\"].to(device, dtype = torch.float)\n",
    "        \n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        # add sigmoid, for the training sigmoid is in BCEWithLogitsLoss\n",
    "        outputs = torch.sigmoid(outputs).detach().cpu()\n",
    "        # thresholding at 0.5\n",
    "        preds = outputs.round()\n",
    "        targets = targets.detach().cpu()\n",
    "\n",
    "        titles.extend(title)\n",
    "        predictions.extend(preds)\n",
    "        prediction_probs.extend(outputs)\n",
    "        target_values.extend(targets)\n",
    "    \n",
    "    predictions = torch.stack(predictions)\n",
    "    prediction_probs = torch.stack(prediction_probs)\n",
    "    target_values = torch.stack(target_values)\n",
    "    \n",
    "    return titles, predictions, prediction_probs, target_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles, predictions, prediction_probs, target_values = get_predictions(model, test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
